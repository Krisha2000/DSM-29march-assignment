{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed083bb4-f21f-4cfa-be9c-83d37f62f1dc",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a11f9d-9d6b-436d-9da6-c618d0c4b662",
   "metadata": {},
   "source": [
    "Lasso Regression, also known as L1 regularization, is a linear regression technique that adds a penalty term to the cost function, which is the sum of squared residuals, similar to Ridge Regression. However, unlike Ridge Regression, Lasso Regression uses L1 regularization, which can shrink some coefficients to exactly zero. This property makes Lasso Regression useful for feature selection, as it automatically selects a subset of the most relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433234a-ccca-408f-9bb0-2843f8774f35",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d2df1-f9bf-4e76-b286-c850e8aed4c0",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression for feature selection is its ability to perform automatic variable selection. By setting some coefficients to zero, Lasso Regression effectively identifies and selects the most important predictors in the model. This can simplify the model, enhance interpretability, and reduce overfitting by excluding irrelevant or redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add155a8-596f-401c-8a3c-e671685a9dc1",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb58251-c256-41b2-b3f3-78f1e5b79980",
   "metadata": {},
   "source": [
    "The interpretation of coefficients in Lasso Regression is similar to other linear regression techniques. Each coefficient represents the expected change in the dependent variable associated with a one-unit change in the corresponding independent variable, assuming all other variables are held constant. However, due to the L1 regularization, some coefficients may be exactly zero, indicating that the corresponding predictors are excluded from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e590747-66e4-40e5-881b-026ec643e5a2",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296cd1b-ce6c-4a9e-859c-ce6b3afccf4e",
   "metadata": {},
   "source": [
    "The tuning parameter in Lasso Regression is typically denoted as lambda (Î») and controls the strength of the regularization. By adjusting lambda, you can control the amount of shrinkage applied to the coefficients. A smaller lambda allows more coefficients to be non-zero, while a larger lambda results in more coefficients being shrunk towards zero or exactly zero. The choice of the tuning parameter depends on the specific problem and the trade-off between model complexity and model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad2658-f2a3-4595-b867-cae4cc5a5f9f",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30026e81-1531-4b70-96dd-e70ad5f01842",
   "metadata": {},
   "source": [
    " Lasso Regression can handle non-linear regression problems to some extent. Although it is a linear regression technique, by incorporating non-linear transformations of the input variables, such as polynomial or interaction terms, Lasso Regression can capture non-linear relationships between the predictors and the dependent variable. However, for more complex non-linear relationships, other regression techniques such as polynomial regression or non-linear regression models may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2634bc-e2da-424e-9801-d41224c630ee",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b299441-8ff4-44a1-9c9f-4e059a3fb3e8",
   "metadata": {},
   "source": [
    "The main difference between Ridge Regression and Lasso Regression lies in the type of regularization they use. Ridge Regression uses L2 regularization, which adds the sum of squared coefficients to the cost function, while Lasso Regression uses L1 regularization, which adds the sum of absolute coefficients to the cost function. This difference leads to different shrinkage properties: Ridge Regression can shrink coefficients towards zero but not exactly to zero, while Lasso Regression can shrink coefficients to exactly zero, performing automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703e5bd-1e3a-4963-bef2-2782481d2e71",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b49d69-cb25-4014-9743-ca66a9cb5f81",
   "metadata": {},
   "source": [
    " Yes, Lasso Regression can handle multicollinearity in the input features. Similar to Ridge Regression, Lasso Regression can help mitigate the effects of multicollinearity by shrinking the coefficients. In the presence of multicollinearity, Lasso Regression tends to select one of the correlated variables and sets the coefficients of the others to zero. This way, it effectively identifies and excludes redundant predictors from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec44245-91d6-4b6e-8fb8-6faf3997f6a9",
   "metadata": {},
   "source": [
    "# Quetion : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd1943-372a-4e97-9aa9-ec72d80cba79",
   "metadata": {},
   "source": [
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using techniques such as cross-validation or grid search. These methods involve evaluating the model's performance on different values of lambda and selecting the one that achieves the best trade-off between bias and variance. The specific choice of lambda depends on the problem at hand and the desired balance between model complexity and model fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
